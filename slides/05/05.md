title: NPFL139, Lecture 5
class: title, langtech, cc-by-sa
# Rainbow II, Distributional RL

## Milan Straka

### March 19, 2025

---
section: $N$-step
class: section
# Multi-step DQN

---
# Rainbow DQN Extensions
## Multi-step DQN

Instead of Q-learning, we use $n$-step variant of Q-learning, which estimates
return as
$$∑_{i=1}^n γ^{i-1} R_i + γ^n \max_{a'} Q(s', a'; →θ̄).$$

~~~
This changes the off-policy algorithm to on-policy (because the “inner” actions
are sampled from the behaviour distribution, but should follow the target distribution);
however, it is not discussed in any way by the authors.

---
section: NoisyNets
class: section
# Noisy Nets

---
# Rainbow DQN Extensions

## Noisy Nets

Noisy Nets are neural networks whose weights and biases are perturbed by
a parametric function of a noise.

~~~
The parameters $→θ$ of a regular neural network are in Noisy nets represented as
$$→θ ≈ →μ + →σ ⊙ →ε,$$
where $→ε$ is zero-mean noise with fixed statistics. We therefore learn the
parameters $(→μ, →σ)$.

~~~
A fully connected layer $→y = →w →x + →b$ with parameters $(→w, →b)$ is
represented in the following way in Noisy nets:
$$→y = (→μ_w + →σ_w ⊙ →ε_w) →x + (→μ_b + →σ_b ⊙ →ε_b).$$

~~~
Each $σ_{i,j}$ is initialized to $\frac{σ_0}{\sqrt{n}}$, where $n$ is the number
of input neurons of the layer in question, and $σ_0$ is a hyperparameter; commonly 0.5.

---
# Rainbow DQN Extensions

## Noisy Nets

The noise $ε$ can be for example independent Gaussian noise. However, for
performance reasons, factorized Gaussian noise is used to generate a matrix of
noise. If $ε_{i, j}$ is noise corresponding to a layer with $n$ inputs and $m$
outputs, we generate independent noise $ε_i$ for input neurons, independent
noise $ε_j$ for output neurons, and set
$$ε_{i,j} = f(ε_i) f(ε_j)~~~\textrm{for}~~~f(x) = \operatorname{sign}(x) \sqrt{|x|}.$$
~~~
The authors generate noise samples for every batch, sharing the noise for all
batch instances (consequently, during loss computation, online and target
network use independent noise).

~~~
### Deep Q Networks
When training a DQN, $ε$-greedy is no longer used (all policies are greedy), and
all fully connected layers are parametrized as noisy nets in both the current
and target network (i.e., networks produce samples from the distribution of
returns, and greedy actions still explore).

---
# Rainbow DQN Extensions

## Noisy Nets

![w=50%,h=center](dqn_noisynets_results.svgz)

![w=65%,h=center](dqn_noisynets_curves.svgz)

---
# Rainbow DQN Extensions

## Noisy Nets

![w=100%](dqn_noisynets_noise_study.svgz)

The $Σ̄$ is the mean-absolute of the noise weights $→σ_w$, i.e.,
$Σ̄ = \frac{1}{\textit{layer size}} \|→σ_w\|_1$.

---
section: DistributionalRL
class: section
# Distributional RL

---
# Rainbow DQN Extensions

## Distributional RL

Instead of an expected return $Q(s, a)$, we could estimate the distribution of
expected returns $Z(s, a)$ – the _value distribution_.

~~~
The authors define the distributional Bellman operator $𝓣^π$ as:
$$𝓣^π Z(s, a) ≝ R(s, a) + γ Z(S', A')~~~\textrm{for}~~~S'∼p(s, a), A'∼π(S').$$

~~~
The authors of the paper prove similar properties of the distributional Bellman
operator compared to the regular Bellman operator, mainly being a contraction
under a suitable metric
~~~
(for Wasserstein metric $W_p$, the authors define
$W̄_p(Z_1, Z_2)≝\sup_{s, a} W_p\big(Z_1(s, a), Z_2(s, a)\big)$ and prove that
$𝓣^π$ is a γ-contraction in $W̄_p$).

---
style: .katex-display { margin: .8em 0 }
class: dbend
# Wasserstein Metric

For two probability distributions $μ, ν$, Wasserstein metric $W_p$ is defined as
$$W_p(μ, ν) ≝ \inf_{γ∈Γ(μ,ν)} \big(𝔼_{(x, y)∼γ} \|x-y\|^d\big)^{1/p},$$
~~~
where $Γ(μ,ν)$ is a set of all _couplings_, each being a joint probability
distribution whose marginals are $μ$ and $ν$, respectively.
~~~
A possible intuition is the optimal transport of probability mass from $μ$ to
$ν$.

~~~
For distributions over reals with CDFs $F, G$, the optimal transport has an
analytic solution:

![w=27.5%,f=right](wasserstein-1.svgz)

$$W_p(μ, ν) = \bigg(∫\nolimits_0^1 |F^{-1}(q) - G^{-1}(q)|^p \d q\bigg)^{1/p},$$
where $F^{-1}$ and $G^{-1}$ are _quantile functions_, i.e., inverse CDFs.

~~~
For $p=1$, the 1-Wasserstein metric correspond to area “between” F and G, and
in that case we can compute it also as $W_1(μ, ν) = ∫\nolimits_x \big|F(x)- G(x)\big| \d x.$

---
# Rainbow DQN Extensions

## Distributional RL

The distribution of returns is modeled as a discrete distribution parametrized
by the number of atoms $N ∈ ℕ$ and by $V_\textrm{MIN}, V_\textrm{MAX} ∈ ℝ$.
Support of the distribution are atoms
$$\{z_i ≝ V_\textrm{MIN} + i Δz : 0 ≤ i < N\}\textrm{~~~for~}Δz ≝ \frac{V_\textrm{MAX} - V_\textrm{MIN}}{N-1}.$$

~~~
The atom probabilities are predicted using a $\softmax$ distribution as
$$Z_{→θ}(s, a) = \left\{z_i\textrm{ with probability }p_i = \frac{e^{f_i(s, a; →θ)}}{∑_j e^{f_j(s, a; →θ)}}\right\}.$$

---
# Rainbow DQN Extensions

## Distributional RL

![w=30%,f=right](dqn_distributional_operator.svgz)

After the Bellman update, the support of the distribution $R(s, a) + γZ(s', a')$
is not the same as the original support. We therefore project it to the original
support by proportionally mapping each atom of the Bellman update to immediate
neighbors in the original support.

~~~
$$Φ\big(R(s, a) + γZ(s', a')\big)_i ≝
  ∑_{j=1}^N \left[ 1 - \frac{\left|[r + γz_j]_{V_\textrm{MIN}}^{V_\textrm{MAX}}-z_i\right|}{Δz} \right]_0^1 p_j(s', a').$$

~~~
The network is trained to minimize the Kullbeck-Leibler divergence between the
current distribution and the (mapped) distribution of the one-step update
$$D_\textrm{KL}\Big(Φ\big(R + γZ_{→θ̄}\big(s', \argmax_{a'} 𝔼Z_{→θ̄}(s', a')\big)\big) \Big\| Z_{→θ}\big(s, a\big)\Big).$$

---
# Rainbow DQN Extensions

## Distributional RL

![w=50%,h=center](dqn_distributional_algorithm.svgz)


---
# Rainbow DQN Extensions

## Distributional RL

![w=40%,h=center](dqn_distributional_results.svgz)

![w=40%,h=center](dqn_distributional_example_distribution.svgz)

---
# Rainbow DQN Extensions

## Distributional RL

![w=100%](dqn_distributional_example_distributions.svgz)

---
# Rainbow DQN Extensions

## Distributional RL

![w=100%](dqn_distributional_atoms_ablation.svgz)
